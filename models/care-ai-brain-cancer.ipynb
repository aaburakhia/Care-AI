{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9537604,"sourceType":"datasetVersion","datasetId":2058865}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================================================================================\n# EXPLORATORY DATA ANALYSIS - BREAST CANCER\n# ==================================================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nfrom collections import Counter\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (15, 10)\n\n# ==================================================================================\n# 1. DATASET OVERVIEW\n# ==================================================================================\n\nprint(\"=\" * 80)\nprint(\"BREAST CANCER DATASET - EXPLORATORY DATA ANALYSIS\")\nprint(\"=\" * 80)\n\n# Define the dataset path\nCANCER_TYPE = 'Breast_Cancer'\nDATASET_PATH = '/kaggle/input/multi-cancer/Multi Cancer/Multi Cancer/Breast Cancer'\n\nprint(f\"\\nğŸ“ Dataset Path: {DATASET_PATH}\")\nprint(f\"ğŸ”¬ Cancer Type: {CANCER_TYPE}\")\n\n# ==================================================================================\n# 2. DIRECTORY STRUCTURE & CLASS DISTRIBUTION\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIRECTORY STRUCTURE & CLASS DISTRIBUTION\")\nprint(\"=\" * 80)\n\nclasses = []\nclass_counts = {}\nall_files = {}\n\n# Check if path exists\nif os.path.exists(DATASET_PATH):\n    classes = os.listdir(DATASET_PATH)\n    classes = [c for c in classes if os.path.isdir(os.path.join(DATASET_PATH, c))]\n    \n    print(f\"\\nâœ“ Found {len(classes)} classes: {classes}\")\n    \n    for class_name in classes:\n        class_path = os.path.join(DATASET_PATH, class_name)\n        files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n        all_files[class_name] = files\n        class_counts[class_name] = len(files)\n        print(f\"  - {class_name}: {len(files)} images\")\n    \n    total_images = sum(class_counts.values())\n    print(f\"\\nğŸ“Š Total Images: {total_images}\")\n    \n    # Calculate class distribution percentages\n    print(\"\\nğŸ“ˆ Class Distribution:\")\n    for class_name, count in class_counts.items():\n        percentage = (count / total_images) * 100\n        print(f\"  - {class_name}: {percentage:.2f}%\")\nelse:\n    print(f\"\\nâŒ Error: Path {DATASET_PATH} not found!\")\n\n# ==================================================================================\n# 3. VISUALIZE CLASS DISTRIBUTION\n# ==================================================================================\n\nif class_counts:\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Bar plot\n    axes[0].bar(class_counts.keys(), class_counts.values(), color=['#FF6B6B', '#4ECDC4'])\n    axes[0].set_xlabel('Class', fontsize=12, fontweight='bold')\n    axes[0].set_ylabel('Number of Images', fontsize=12, fontweight='bold')\n    axes[0].set_title('Class Distribution - Bar Chart', fontsize=14, fontweight='bold')\n    axes[0].grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for i, (class_name, count) in enumerate(class_counts.items()):\n        axes[0].text(i, count + 50, str(count), ha='center', va='bottom', fontweight='bold')\n    \n    # Pie chart\n    colors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#F38181', '#AA96DA']\n    axes[1].pie(class_counts.values(), labels=class_counts.keys(), autopct='%1.1f%%',\n                startangle=90, colors=colors[:len(class_counts)], textprops={'fontsize': 12, 'fontweight': 'bold'})\n    axes[1].set_title('Class Distribution - Pie Chart', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ==================================================================================\n# 4. SAMPLE IMAGES FROM EACH CLASS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE IMAGES FROM EACH CLASS\")\nprint(\"=\" * 80)\n\nif all_files:\n    num_samples = 5\n    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4 * len(classes)))\n    \n    if len(classes) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for idx, class_name in enumerate(classes):\n        # Get random sample of images\n        sample_files = np.random.choice(all_files[class_name], \n                                       min(num_samples, len(all_files[class_name])), \n                                       replace=False)\n        \n        for img_idx, img_file in enumerate(sample_files):\n            img_path = os.path.join(DATASET_PATH, class_name, img_file)\n            img = Image.open(img_path)\n            \n            axes[idx, img_idx].imshow(img)\n            axes[idx, img_idx].axis('off')\n            \n            if img_idx == 0:\n                axes[idx, img_idx].set_title(f'{class_name}\\n{img.size[0]}x{img.size[1]}', \n                                            fontsize=12, fontweight='bold', loc='left')\n            else:\n                axes[idx, img_idx].set_title(f'{img.size[0]}x{img.size[1]}', \n                                            fontsize=10)\n    \n    plt.suptitle(f'{CANCER_TYPE} - Sample Images', fontsize=16, fontweight='bold', y=1.00)\n    plt.tight_layout()\n    plt.show()\n\n# ==================================================================================\n# 5. IMAGE DIMENSIONS ANALYSIS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"IMAGE DIMENSIONS ANALYSIS\")\nprint(\"=\" * 80)\n\nif all_files:\n    dimensions = {class_name: [] for class_name in classes}\n    aspect_ratios = {class_name: [] for class_name in classes}\n    file_sizes = {class_name: [] for class_name in classes}\n    \n    # Sample images for analysis (analyze 100 images per class for speed)\n    sample_size = min(100, min([len(files) for files in all_files.values()]))\n    \n    print(f\"\\nğŸ” Analyzing {sample_size} images per class...\")\n    \n    for class_name in classes:\n        sample_files = np.random.choice(all_files[class_name], sample_size, replace=False)\n        \n        for img_file in sample_files:\n            img_path = os.path.join(DATASET_PATH, class_name, img_file)\n            try:\n                img = Image.open(img_path)\n                width, height = img.size\n                dimensions[class_name].append((width, height))\n                aspect_ratios[class_name].append(width / height)\n                file_sizes[class_name].append(os.path.getsize(img_path) / 1024)  # KB\n            except Exception as e:\n                print(f\"Error reading {img_file}: {e}\")\n    \n    # Display statistics\n    print(\"\\nğŸ“ Dimension Statistics:\")\n    for class_name in classes:\n        widths = [d[0] for d in dimensions[class_name]]\n        heights = [d[1] for d in dimensions[class_name]]\n        \n        print(f\"\\n  {class_name}:\")\n        print(f\"    - Width:  min={min(widths)}, max={max(widths)}, mean={np.mean(widths):.1f}, std={np.std(widths):.1f}\")\n        print(f\"    - Height: min={min(heights)}, max={max(heights)}, mean={np.mean(heights):.1f}, std={np.std(heights):.1f}\")\n        print(f\"    - Aspect Ratio: mean={np.mean(aspect_ratios[class_name]):.3f}, std={np.std(aspect_ratios[class_name]):.3f}\")\n        print(f\"    - File Size (KB): min={min(file_sizes[class_name]):.1f}, max={max(file_sizes[class_name]):.1f}, mean={np.mean(file_sizes[class_name]):.1f}\")\n    \n    # Visualize dimensions distribution\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Width distribution\n    for class_name in classes:\n        widths = [d[0] for d in dimensions[class_name]]\n        axes[0, 0].hist(widths, bins=30, alpha=0.6, label=class_name)\n    axes[0, 0].set_xlabel('Width (pixels)', fontweight='bold')\n    axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n    axes[0, 0].set_title('Image Width Distribution', fontweight='bold')\n    axes[0, 0].legend()\n    axes[0, 0].grid(alpha=0.3)\n    \n    # Height distribution\n    for class_name in classes:\n        heights = [d[1] for d in dimensions[class_name]]\n        axes[0, 1].hist(heights, bins=30, alpha=0.6, label=class_name)\n    axes[0, 1].set_xlabel('Height (pixels)', fontweight='bold')\n    axes[0, 1].set_ylabel('Frequency', fontweight='bold')\n    axes[0, 1].set_title('Image Height Distribution', fontweight='bold')\n    axes[0, 1].legend()\n    axes[0, 1].grid(alpha=0.3)\n    \n    # Aspect ratio distribution\n    for class_name in classes:\n        axes[1, 0].hist(aspect_ratios[class_name], bins=30, alpha=0.6, label=class_name)\n    axes[1, 0].set_xlabel('Aspect Ratio (Width/Height)', fontweight='bold')\n    axes[1, 0].set_ylabel('Frequency', fontweight='bold')\n    axes[1, 0].set_title('Aspect Ratio Distribution', fontweight='bold')\n    axes[1, 0].legend()\n    axes[1, 0].grid(alpha=0.3)\n    \n    # File size distribution\n    for class_name in classes:\n        axes[1, 1].hist(file_sizes[class_name], bins=30, alpha=0.6, label=class_name)\n    axes[1, 1].set_xlabel('File Size (KB)', fontweight='bold')\n    axes[1, 1].set_ylabel('Frequency', fontweight='bold')\n    axes[1, 1].set_title('File Size Distribution', fontweight='bold')\n    axes[1, 1].legend()\n    axes[1, 1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# ==================================================================================\n# 6. COLOR CHANNEL ANALYSIS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COLOR CHANNEL ANALYSIS\")\nprint(\"=\" * 80)\n\nif all_files:\n    print(f\"\\nğŸ¨ Analyzing color channels for {sample_size} images per class...\")\n    \n    color_stats = {class_name: {'R': [], 'G': [], 'B': []} for class_name in classes}\n    \n    for class_name in classes:\n        sample_files = np.random.choice(all_files[class_name], min(50, len(all_files[class_name])), replace=False)\n        \n        for img_file in sample_files:\n            img_path = os.path.join(DATASET_PATH, class_name, img_file)\n            try:\n                img = cv2.imread(img_path)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                \n                color_stats[class_name]['R'].append(np.mean(img[:, :, 0]))\n                color_stats[class_name]['G'].append(np.mean(img[:, :, 1]))\n                color_stats[class_name]['B'].append(np.mean(img[:, :, 2]))\n            except Exception as e:\n                continue\n    \n    # Visualize color channel distributions\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    channels = ['R', 'G', 'B']\n    channel_colors = ['red', 'green', 'blue']\n    \n    for idx, (channel, color) in enumerate(zip(channels, channel_colors)):\n        for class_name in classes:\n            axes[idx].hist(color_stats[class_name][channel], bins=30, alpha=0.6, label=class_name)\n        axes[idx].set_xlabel(f'{channel} Channel Mean Intensity', fontweight='bold')\n        axes[idx].set_ylabel('Frequency', fontweight='bold')\n        axes[idx].set_title(f'{channel} Channel Distribution', fontweight='bold', color=color)\n        axes[idx].legend()\n        axes[idx].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print color statistics\n    print(\"\\nğŸ“Š Color Channel Statistics:\")\n    for class_name in classes:\n        print(f\"\\n  {class_name}:\")\n        for channel in channels:\n            mean_val = np.mean(color_stats[class_name][channel])\n            std_val = np.std(color_stats[class_name][channel])\n            print(f\"    - {channel} Channel: mean={mean_val:.2f}, std={std_val:.2f}\")\n\n# ==================================================================================\n# 7. SUMMARY & RECOMMENDATIONS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SUMMARY & RECOMMENDATIONS\")\nprint(\"=\" * 80)\n\nif class_counts:\n    print(\"\\nâœ… Dataset Summary:\")\n    print(f\"  - Total Images: {sum(class_counts.values())}\")\n    print(f\"  - Number of Classes: {len(classes)}\")\n    print(f\"  - Classes: {', '.join(classes)}\")\n    \n    # Check for class imbalance\n    if class_counts:\n        max_count = max(class_counts.values())\n        min_count = min(class_counts.values())\n        imbalance_ratio = max_count / min_count if min_count > 0 else 0\n        \n        print(f\"\\nâš–ï¸ Class Balance:\")\n        if imbalance_ratio > 1.5:\n            print(f\"  - âš ï¸ Class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n            print(f\"  - Recommendation: Consider using class weights or data augmentation\")\n        else:\n            print(f\"  - âœ“ Classes are relatively balanced (ratio: {imbalance_ratio:.2f})\")\n    \n    print(f\"\\nğŸ¯ Recommended Image Size for Training: 224x224 (standard for transfer learning)\")\n    print(f\"ğŸ“¦ Recommended Batch Size: 32\")\n    print(f\"ğŸ”„ Data Augmentation: Recommended to improve model generalization\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EDA COMPLETED!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T00:40:25.235800Z","iopub.execute_input":"2025-10-28T00:40:25.236277Z","iopub.status.idle":"2025-10-28T00:40:39.259343Z","shell.execute_reply.started":"2025-10-28T00:40:25.236255Z","shell.execute_reply":"2025-10-28T00:40:39.258566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# INSTALL ONNX CONVERSION TOOLS\n# ==================================================================================\n\nprint(\"Installing ONNX conversion tools...\")\n!pip install -q tf2onnx onnx onnxruntime\n\nprint(\"âœ… Installation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T00:49:52.414301Z","iopub.execute_input":"2025-10-28T00:49:52.415243Z","iopub.status.idle":"2025-10-28T00:50:11.228656Z","shell.execute_reply.started":"2025-10-28T00:49:52.415209Z","shell.execute_reply":"2025-10-28T00:50:11.227688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# DATA PREPARATION & PREPROCESSING\n# ==================================================================================\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\" * 80)\nprint(\"DATA PREPARATION - BREAST CANCER CLASSIFIER\")\nprint(\"=\" * 80)\n\n# Configuration\nCANCER_TYPE = 'Breast_Cancer'\nDATASET_PATH = '/kaggle/input/multi-cancer/Multi Cancer/Multi Cancer/Breast Cancer'\nIMG_SIZE = (224, 224)  # Standard size for EfficientNet\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\nRANDOM_SEED = 42\n\n# Set seeds for reproducibility\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\nprint(f\"\\nğŸ“Š Configuration:\")\nprint(f\"  - Image Size: {IMG_SIZE}\")\nprint(f\"  - Batch Size: {BATCH_SIZE}\")\nprint(f\"  - Validation Split: {VALIDATION_SPLIT * 100}%\")\nprint(f\"  - Random Seed: {RANDOM_SEED}\")\n\n# ==================================================================================\n# DATA AUGMENTATION\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DATA AUGMENTATION SETUP\")\nprint(\"=\" * 80)\n\n# Training data augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    zoom_range=0.15,\n    shear_range=0.15,\n    fill_mode='nearest',\n    validation_split=VALIDATION_SPLIT\n)\n\n# Validation data - only rescaling\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=VALIDATION_SPLIT\n)\n\nprint(\"âœ… Augmentation strategies:\")\nprint(\"  - Rotation: Â±20Â°\")\nprint(\"  - Horizontal & Vertical Flip\")\nprint(\"  - Width/Height Shift: 20%\")\nprint(\"  - Zoom: 15%\")\nprint(\"  - Shear: 15%\")\n\n# ==================================================================================\n# DATA GENERATORS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING DATA GENERATORS\")\nprint(\"=\" * 80)\n\n# Training generator\ntrain_generator = train_datagen.flow_from_directory(\n    DATASET_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training',\n    shuffle=True,\n    seed=RANDOM_SEED\n)\n\n# Validation generator\nvalidation_generator = val_datagen.flow_from_directory(\n    DATASET_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation',\n    shuffle=False,\n    seed=RANDOM_SEED\n)\n\nprint(f\"\\nâœ… Data Generators Created:\")\nprint(f\"  - Training samples: {train_generator.samples}\")\nprint(f\"  - Validation samples: {validation_generator.samples}\")\nprint(f\"  - Classes: {train_generator.class_indices}\")\nprint(f\"  - Steps per epoch (train): {train_generator.samples // BATCH_SIZE}\")\nprint(f\"  - Validation steps: {validation_generator.samples // BATCH_SIZE}\")\n\n# ==================================================================================\n# VISUALIZE AUGMENTED SAMPLES\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE AUGMENTED IMAGES\")\nprint(\"=\" * 80)\n\n# Get a batch of augmented images\nsample_batch = train_generator.next()\nsample_images = sample_batch[0][:6]\nsample_labels = sample_batch[1][:6]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nclass_names = {v: k for k, v in train_generator.class_indices.items()}\n\nfor idx in range(6):\n    axes[idx].imshow(sample_images[idx])\n    label = \"Malignant\" if sample_labels[idx] > 0.5 else \"Benign\"\n    axes[idx].set_title(f'{label}', fontsize=12, fontweight='bold')\n    axes[idx].axis('off')\n\nplt.suptitle('Sample Augmented Training Images', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Data preparation completed!\")\nprint(\"Ready for model training...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T00:50:56.562575Z","iopub.execute_input":"2025-10-28T00:50:56.563338Z","iopub.status.idle":"2025-10-28T00:51:08.165601Z","shell.execute_reply.started":"2025-10-28T00:50:56.563307Z","shell.execute_reply":"2025-10-28T00:51:08.164660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# VISUALIZE AUGMENTED SAMPLES\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE AUGMENTED IMAGES\")\nprint(\"=\" * 80)\n\n# Get a batch of augmented images (fixed method)\nsample_batch = next(train_generator)\nsample_images = sample_batch[0][:6]\nsample_labels = sample_batch[1][:6]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nclass_names = {v: k for k, v in train_generator.class_indices.items()}\n\nfor idx in range(6):\n    axes[idx].imshow(sample_images[idx])\n    label = \"Malignant\" if sample_labels[idx] > 0.5 else \"Benign\"\n    axes[idx].set_title(f'{label}', fontsize=12, fontweight='bold')\n    axes[idx].axis('off')\n\nplt.suptitle('Sample Augmented Training Images', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Data preparation completed!\")\nprint(\"Ready for model training...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T00:56:05.741389Z","iopub.execute_input":"2025-10-28T00:56:05.741692Z","iopub.status.idle":"2025-10-28T00:56:08.511958Z","shell.execute_reply.started":"2025-10-28T00:56:05.741673Z","shell.execute_reply":"2025-10-28T00:56:08.510865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# MODEL BUILDING - EFFICIENTNET WITH TRANSFER LEARNING\n# ==================================================================================\n\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nprint(\"=\" * 80)\nprint(\"MODEL BUILDING - BREAST CANCER CLASSIFIER\")\nprint(\"=\" * 80)\n\n# ==================================================================================\n# BUILD MODEL\n# ==================================================================================\n\ndef build_breast_cancer_model(img_size=(224, 224, 3)):\n    \"\"\"\n    Build a binary classification model using EfficientNetB3 with transfer learning\n    \"\"\"\n    print(\"\\nğŸ—ï¸ Building model architecture...\")\n    \n    # Load pre-trained EfficientNetB3\n    base_model = EfficientNetB3(\n        include_top=False,\n        weights='imagenet',\n        input_shape=img_size,\n        pooling='avg'\n    )\n    \n    # Freeze base model layers initially\n    base_model.trainable = False\n    \n    # Build the model\n    inputs = layers.Input(shape=img_size, name='input_layer')\n    \n    # Base model\n    x = base_model(inputs, training=False)\n    \n    # Classification head\n    x = layers.Dropout(0.3, name='dropout_1')(x)\n    x = layers.Dense(256, activation='relu', name='dense_1')(x)\n    x = layers.BatchNormalization(name='batch_norm_1')(x)\n    x = layers.Dropout(0.3, name='dropout_2')(x)\n    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n    x = layers.BatchNormalization(name='batch_norm_2')(x)\n    x = layers.Dropout(0.2, name='dropout_3')(x)\n    \n    # Output layer (sigmoid for binary classification)\n    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name='breast_cancer_classifier')\n    \n    return model, base_model\n\n# Build the model\nmodel, base_model = build_breast_cancer_model(img_size=(224, 224, 3))\n\nprint(f\"\\nâœ… Model built successfully!\")\nprint(f\"  - Total layers: {len(model.layers)}\")\nprint(f\"  - Trainable layers: {sum([1 for layer in model.layers if layer.trainable])}\")\n\n# ==================================================================================\n# COMPILE MODEL\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPILING MODEL\")\nprint(\"=\" * 80)\n\n# Compile with initial settings\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=1e-3),\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nprint(\"âœ… Model compiled with:\")\nprint(\"  - Optimizer: Adam (lr=0.001)\")\nprint(\"  - Loss: Binary Crossentropy\")\nprint(\"  - Metrics: Accuracy, Precision, Recall, AUC\")\n\n# Display model summary\nprint(\"\\nğŸ“‹ Model Summary:\")\nmodel.summary()\n\n# ==================================================================================\n# CALLBACKS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SETTING UP CALLBACKS\")\nprint(\"=\" * 80)\n\n# Early stopping\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Reduce learning rate on plateau\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    min_lr=1e-7,\n    verbose=1\n)\n\n# Model checkpoint\ncheckpoint = ModelCheckpoint(\n    'best_breast_cancer_model.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    verbose=1\n)\n\ncallbacks = [early_stop, reduce_lr, checkpoint]\n\nprint(\"âœ… Callbacks configured:\")\nprint(\"  - Early Stopping (patience=5)\")\nprint(\"  - Reduce LR on Plateau (patience=3, factor=0.5)\")\nprint(\"  - Model Checkpoint (save best model)\")\n\nprint(\"\\nâœ… Ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T00:56:28.059352Z","iopub.execute_input":"2025-10-28T00:56:28.059636Z","iopub.status.idle":"2025-10-28T00:56:34.642483Z","shell.execute_reply.started":"2025-10-28T00:56:28.059617Z","shell.execute_reply":"2025-10-28T00:56:34.641851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# MODEL TRAINING - PHASE 1: TRAIN WITH FROZEN BASE\n# ==================================================================================\n\nprint(\"=\" * 80)\nprint(\"TRAINING PHASE 1: FROZEN BASE MODEL\")\nprint(\"=\" * 80)\n\n# Training configuration\nEPOCHS_PHASE1 = 8\n\nprint(f\"\\nğŸ¯ Training Configuration:\")\nprint(f\"  - Epochs: {EPOCHS_PHASE1}\")\nprint(f\"  - Training samples: {train_generator.samples}\")\nprint(f\"  - Validation samples: {validation_generator.samples}\")\nprint(f\"  - Steps per epoch: {train_generator.samples // BATCH_SIZE}\")\nprint(f\"  - Validation steps: {validation_generator.samples // BATCH_SIZE}\")\n\nprint(\"\\nğŸš€ Starting training...\")\nprint(\"-\" * 80)\n\n# Train the model\nhistory_phase1 = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=EPOCHS_PHASE1,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\nâœ… Phase 1 training completed!\")\n\n# ==================================================================================\n# TRAINING RESULTS - PHASE 1\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PHASE 1 TRAINING RESULTS\")\nprint(\"=\" * 80)\n\n# Get final metrics\nfinal_train_acc = history_phase1.history['accuracy'][-1]\nfinal_val_acc = history_phase1.history['val_accuracy'][-1]\nfinal_train_loss = history_phase1.history['loss'][-1]\nfinal_val_loss = history_phase1.history['val_loss'][-1]\n\nprint(f\"\\nğŸ“Š Final Metrics:\")\nprint(f\"  - Training Accuracy: {final_train_acc:.4f}\")\nprint(f\"  - Validation Accuracy: {final_val_acc:.4f}\")\nprint(f\"  - Training Loss: {final_train_loss:.4f}\")\nprint(f\"  - Validation Loss: {final_val_loss:.4f}\")\n\n# Best metrics\nbest_val_acc = max(history_phase1.history['val_accuracy'])\nbest_epoch = history_phase1.history['val_accuracy'].index(best_val_acc) + 1\n\nprint(f\"\\nğŸ† Best Performance:\")\nprint(f\"  - Best Validation Accuracy: {best_val_acc:.4f}\")\nprint(f\"  - Best Epoch: {best_epoch}\")\n\n# ==================================================================================\n# VISUALIZE TRAINING HISTORY - PHASE 1\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING HISTORY VISUALIZATION\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Accuracy plot\naxes[0, 0].plot(history_phase1.history['accuracy'], label='Training Accuracy', linewidth=2)\naxes[0, 0].plot(history_phase1.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\naxes[0, 0].set_xlabel('Epoch', fontweight='bold')\naxes[0, 0].set_ylabel('Accuracy', fontweight='bold')\naxes[0, 0].set_title('Model Accuracy - Phase 1', fontweight='bold', fontsize=14)\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# Loss plot\naxes[0, 1].plot(history_phase1.history['loss'], label='Training Loss', linewidth=2)\naxes[0, 1].plot(history_phase1.history['val_loss'], label='Validation Loss', linewidth=2)\naxes[0, 1].set_xlabel('Epoch', fontweight='bold')\naxes[0, 1].set_ylabel('Loss', fontweight='bold')\naxes[0, 1].set_title('Model Loss - Phase 1', fontweight='bold', fontsize=14)\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\n\n# Precision plot\naxes[1, 0].plot(history_phase1.history['precision'], label='Training Precision', linewidth=2)\naxes[1, 0].plot(history_phase1.history['val_precision'], label='Validation Precision', linewidth=2)\naxes[1, 0].set_xlabel('Epoch', fontweight='bold')\naxes[1, 0].set_ylabel('Precision', fontweight='bold')\naxes[1, 0].set_title('Model Precision - Phase 1', fontweight='bold', fontsize=14)\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# Recall plot\naxes[1, 1].plot(history_phase1.history['recall'], label='Training Recall', linewidth=2)\naxes[1, 1].plot(history_phase1.history['val_recall'], label='Validation Recall', linewidth=2)\naxes[1, 1].set_xlabel('Epoch', fontweight='bold')\naxes[1, 1].set_ylabel('Recall', fontweight='bold')\naxes[1, 1].set_title('Model Recall - Phase 1', fontweight='bold', fontsize=14)\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\n\nplt.suptitle('Breast Cancer Classifier - Training History (Phase 1)', \n             fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Training visualization completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T01:21:11.773679Z","iopub.execute_input":"2025-10-28T01:21:11.773957Z","iopub.status.idle":"2025-10-28T01:40:32.704718Z","shell.execute_reply.started":"2025-10-28T01:21:11.773939Z","shell.execute_reply":"2025-10-28T01:40:32.703667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# MODEL TRAINING - PHASE 2: FINE-TUNING\n# ==================================================================================\n\nprint(\"=\" * 80)\nprint(\"TRAINING PHASE 2: FINE-TUNING\")\nprint(\"=\" * 80)\n\n# ==================================================================================\n# UNFREEZE BASE MODEL LAYERS\n# ==================================================================================\n\nprint(\"\\nğŸ”“ Unfreezing base model layers for fine-tuning...\")\n\n# Unfreeze the base model\nbase_model.trainable = True\n\n# Freeze the first 80% of layers, fine-tune the last 20%\nfine_tune_at = int(len(base_model.layers) * 0.8)\n\n# Freeze all layers before fine_tune_at\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = False\n\n# Count trainable parameters\ntrainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\nnon_trainable_params = sum([tf.size(w).numpy() for w in model.non_trainable_weights])\n\nprint(f\"\\nâœ… Base model partially unfrozen:\")\nprint(f\"  - Total base layers: {len(base_model.layers)}\")\nprint(f\"  - Frozen layers: {fine_tune_at}\")\nprint(f\"  - Trainable layers: {len(base_model.layers) - fine_tune_at}\")\nprint(f\"  - Trainable parameters: {trainable_params:,}\")\nprint(f\"  - Non-trainable parameters: {non_trainable_params:,}\")\n\n# ==================================================================================\n# RECOMPILE WITH LOWER LEARNING RATE\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RECOMPILING MODEL\")\nprint(\"=\" * 80)\n\n# Compile with lower learning rate for fine-tuning\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=1e-5),  # Much lower learning rate\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nprint(\"âœ… Model recompiled with:\")\nprint(\"  - Optimizer: Adam (lr=0.00001)\")\nprint(\"  - Loss: Binary Crossentropy\")\nprint(\"  - Metrics: Accuracy, Precision, Recall, AUC\")\n\n# ==================================================================================\n# TRAIN PHASE 2\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\" * 80)\n\nEPOCHS_PHASE2 = 10\ntotal_epochs = EPOCHS_PHASE1 + EPOCHS_PHASE2\n\nprint(f\"\\nğŸ¯ Fine-tuning Configuration:\")\nprint(f\"  - Additional Epochs: {EPOCHS_PHASE2}\")\nprint(f\"  - Total Epochs: {total_epochs}\")\nprint(f\"  - Learning Rate: 0.00001\")\n\nprint(\"\\nğŸš€ Starting fine-tuning...\")\nprint(\"-\" * 80)\n\n# Continue training\nhistory_phase2 = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=total_epochs,\n    initial_epoch=len(history_phase1.history['accuracy']),\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\nâœ… Phase 2 fine-tuning completed!\")\n\n# ==================================================================================\n# COMBINE TRAINING HISTORIES\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMBINING TRAINING HISTORIES\")\nprint(\"=\" * 80)\n\n# Combine both training phases\ncombined_history = {\n    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss'],\n    'precision': history_phase1.history['precision'] + history_phase2.history['precision'],\n    'val_precision': history_phase1.history['val_precision'] + history_phase2.history['val_precision'],\n    'recall': history_phase1.history['recall'] + history_phase2.history['recall'],\n    'val_recall': history_phase1.history['val_recall'] + history_phase2.history['val_recall'],\n    'auc': history_phase1.history['auc'] + history_phase2.history['auc'],\n    'val_auc': history_phase1.history['val_auc'] + history_phase2.history['val_auc']\n}\n\n# ==================================================================================\n# FINAL TRAINING RESULTS\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPLETE TRAINING RESULTS\")\nprint(\"=\" * 80)\n\n# Get final metrics\nfinal_train_acc = combined_history['accuracy'][-1]\nfinal_val_acc = combined_history['val_accuracy'][-1]\nfinal_train_loss = combined_history['loss'][-1]\nfinal_val_loss = combined_history['val_loss'][-1]\nfinal_val_precision = combined_history['val_precision'][-1]\nfinal_val_recall = combined_history['val_recall'][-1]\nfinal_val_auc = combined_history['val_auc'][-1]\n\nprint(f\"\\nğŸ“Š Final Metrics:\")\nprint(f\"  - Training Accuracy: {final_train_acc:.4f}\")\nprint(f\"  - Validation Accuracy: {final_val_acc:.4f}\")\nprint(f\"  - Training Loss: {final_train_loss:.4f}\")\nprint(f\"  - Validation Loss: {final_val_loss:.4f}\")\nprint(f\"  - Validation Precision: {final_val_precision:.4f}\")\nprint(f\"  - Validation Recall: {final_val_recall:.4f}\")\nprint(f\"  - Validation AUC: {final_val_auc:.4f}\")\n\n# Calculate F1 Score\nf1_score = 2 * (final_val_precision * final_val_recall) / (final_val_precision + final_val_recall)\nprint(f\"  - Validation F1-Score: {f1_score:.4f}\")\n\n# Best metrics\nbest_val_acc = max(combined_history['val_accuracy'])\nbest_epoch = combined_history['val_accuracy'].index(best_val_acc) + 1\n\nprint(f\"\\nğŸ† Best Performance:\")\nprint(f\"  - Best Validation Accuracy: {best_val_acc:.4f}\")\nprint(f\"  - Best Epoch: {best_epoch}\")\n\n# ==================================================================================\n# VISUALIZE COMPLETE TRAINING HISTORY\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPLETE TRAINING HISTORY VISUALIZATION\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# Accuracy plot\naxes[0, 0].plot(combined_history['accuracy'], label='Training Accuracy', linewidth=2)\naxes[0, 0].plot(combined_history['val_accuracy'], label='Validation Accuracy', linewidth=2)\naxes[0, 0].axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts', alpha=0.7)\naxes[0, 0].set_xlabel('Epoch', fontweight='bold')\naxes[0, 0].set_ylabel('Accuracy', fontweight='bold')\naxes[0, 0].set_title('Model Accuracy - Complete Training', fontweight='bold', fontsize=14)\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# Loss plot\naxes[0, 1].plot(combined_history['loss'], label='Training Loss', linewidth=2)\naxes[0, 1].plot(combined_history['val_loss'], label='Validation Loss', linewidth=2)\naxes[0, 1].axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts', alpha=0.7)\naxes[0, 1].set_xlabel('Epoch', fontweight='bold')\naxes[0, 1].set_ylabel('Loss', fontweight='bold')\naxes[0, 1].set_title('Model Loss - Complete Training', fontweight='bold', fontsize=14)\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\n\n# Precision plot\naxes[0, 2].plot(combined_history['precision'], label='Training Precision', linewidth=2)\naxes[0, 2].plot(combined_history['val_precision'], label='Validation Precision', linewidth=2)\naxes[0, 2].axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts', alpha=0.7)\naxes[0, 2].set_xlabel('Epoch', fontweight='bold')\naxes[0, 2].set_ylabel('Precision', fontweight='bold')\naxes[0, 2].set_title('Model Precision - Complete Training', fontweight='bold', fontsize=14)\naxes[0, 2].legend()\naxes[0, 2].grid(alpha=0.3)\n\n# Recall plot\naxes[1, 0].plot(combined_history['recall'], label='Training Recall', linewidth=2)\naxes[1, 0].plot(combined_history['val_recall'], label='Validation Recall', linewidth=2)\naxes[1, 0].axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts', alpha=0.7)\naxes[1, 0].set_xlabel('Epoch', fontweight='bold')\naxes[1, 0].set_ylabel('Recall', fontweight='bold')\naxes[1, 0].set_title('Model Recall - Complete Training', fontweight='bold', fontsize=14)\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# AUC plot\naxes[1, 1].plot(combined_history['auc'], label='Training AUC', linewidth=2)\naxes[1, 1].plot(combined_history['val_auc'], label='Validation AUC', linewidth=2)\naxes[1, 1].axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts', alpha=0.7)\naxes[1, 1].set_xlabel('Epoch', fontweight='bold')\naxes[1, 1].set_ylabel('AUC', fontweight='bold')\naxes[1, 1].set_title('Model AUC - Complete Training', fontweight='bold', fontsize=14)\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\n\n# Summary metrics\naxes[1, 2].axis('off')\nsummary_text = f\"\"\"\nFINAL PERFORMANCE SUMMARY\n\nValidation Metrics:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nAccuracy:    {final_val_acc:.4f}\nPrecision:   {final_val_precision:.4f}\nRecall:      {final_val_recall:.4f}\nF1-Score:    {f1_score:.4f}\nAUC:         {final_val_auc:.4f}\n\nBest Validation Accuracy:\n{best_val_acc:.4f} (Epoch {best_epoch})\n\nTraining Configuration:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nPhase 1: {EPOCHS_PHASE1} epochs\nPhase 2: {EPOCHS_PHASE2} epochs\nTotal: {total_epochs} epochs\n\"\"\"\naxes[1, 2].text(0.1, 0.5, summary_text, fontsize=12, family='monospace',\n                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.suptitle('Breast Cancer Classifier - Complete Training History', \n             fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Complete training visualization finished!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T01:43:28.645758Z","iopub.execute_input":"2025-10-28T01:43:28.646204Z","iopub.status.idle":"2025-10-28T02:08:46.739379Z","shell.execute_reply.started":"2025-10-28T01:43:28.646180Z","shell.execute_reply":"2025-10-28T02:08:46.738579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# MODEL EVALUATION\n# ==================================================================================\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport seaborn as sns\n\nprint(\"=\" * 80)\nprint(\"MODEL EVALUATION\")\nprint(\"=\" * 80)\n\n# ==================================================================================\n# PREDICTIONS ON VALIDATION SET\n# ==================================================================================\n\nprint(\"\\nğŸ” Generating predictions on validation set...\")\n\n# Reset validation generator\nvalidation_generator.reset()\n\n# Get predictions\ny_pred_probs = model.predict(validation_generator, verbose=1)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten()\n\n# Get true labels\ny_true = validation_generator.classes\n\nprint(f\"\\nâœ… Predictions generated:\")\nprint(f\"  - Total samples: {len(y_true)}\")\nprint(f\"  - Predicted Benign: {np.sum(y_pred == 0)}\")\nprint(f\"  - Predicted Malignant: {np.sum(y_pred == 1)}\")\n\n# ==================================================================================\n# CONFUSION MATRIX\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\" * 80)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Calculate metrics from confusion matrix\ntn, fp, fn, tp = cm.ravel()\n\nprint(f\"\\nBreakdown:\")\nprint(f\"  - True Negatives (TN): {tn}\")\nprint(f\"  - False Positives (FP): {fp}\")\nprint(f\"  - False Negatives (FN): {fn}\")\nprint(f\"  - True Positives (TP): {tp}\")\n\n# Calculate additional metrics\nsensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\nspecificity = tn / (tn + fp) if (tn + fp) > 0 else 0\nppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\nnpv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n\nprint(f\"\\nAdditional Metrics:\")\nprint(f\"  - Sensitivity (Recall): {sensitivity:.4f}\")\nprint(f\"  - Specificity: {specificity:.4f}\")\nprint(f\"  - Positive Predictive Value (Precision): {ppv:.4f}\")\nprint(f\"  - Negative Predictive Value: {npv:.4f}\")\n\n# Visualize confusion matrix\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Confusion matrix - raw counts\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Benign', 'Malignant'],\n            yticklabels=['Benign', 'Malignant'],\n            ax=axes[0], cbar_kws={'label': 'Count'})\naxes[0].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\naxes[0].set_ylabel('True Label', fontweight='bold', fontsize=12)\naxes[0].set_title('Confusion Matrix - Raw Counts', fontweight='bold', fontsize=14)\n\n# Confusion matrix - normalized\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n            xticklabels=['Benign', 'Malignant'],\n            yticklabels=['Benign', 'Malignant'],\n            ax=axes[1], cbar_kws={'label': 'Percentage'})\naxes[1].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\naxes[1].set_ylabel('True Label', fontweight='bold', fontsize=12)\naxes[1].set_title('Confusion Matrix - Normalized', fontweight='bold', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# ==================================================================================\n# CLASSIFICATION REPORT\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\" * 80)\n\n# Get class names\nclass_names = ['Benign', 'Malignant']\n\n# Generate classification report\nreport = classification_report(y_true, y_pred, target_names=class_names, digits=4)\nprint(\"\\n\" + report)\n\n# ==================================================================================\n# ROC CURVE AND AUC\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ROC CURVE\")\nprint(\"=\" * 80)\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\nroc_auc = auc(fpr, tpr)\n\nprint(f\"\\nâœ… ROC AUC Score: {roc_auc:.4f}\")\n\n# Plot ROC curve\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontweight='bold', fontsize=12)\nplt.ylabel('True Positive Rate', fontweight='bold', fontsize=12)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontweight='bold', fontsize=14)\nplt.legend(loc=\"lower right\", fontsize=12)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# ==================================================================================\n# PREDICTION SAMPLES\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE PREDICTIONS\")\nprint(\"=\" * 80)\n\n# Reset validation generator\nvalidation_generator.reset()\n\n# Get a batch\nsample_batch = next(validation_generator)\nsample_images = sample_batch[0][:6]\nsample_labels = sample_batch[1][:6]\n\n# Make predictions\nsample_predictions = model.predict(sample_images, verbose=0)\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfor idx in range(6):\n    axes[idx].imshow(sample_images[idx])\n    \n    true_label = \"Malignant\" if sample_labels[idx] > 0.5 else \"Benign\"\n    pred_label = \"Malignant\" if sample_predictions[idx] > 0.5 else \"Benign\"\n    confidence = sample_predictions[idx][0] if sample_predictions[idx] > 0.5 else 1 - sample_predictions[idx][0]\n    \n    # Color code: green for correct, red for incorrect\n    color = 'green' if true_label == pred_label else 'red'\n    \n    title = f'True: {true_label}\\nPred: {pred_label} ({confidence:.2%})'\n    axes[idx].set_title(title, fontsize=11, fontweight='bold', color=color)\n    axes[idx].axis('off')\n\nplt.suptitle('Sample Predictions (Green = Correct, Red = Incorrect)', \n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# ==================================================================================\n# EVALUATION SUMMARY\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘           BREAST CANCER CLASSIFIER - FINAL RESULTS         â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘  Validation Accuracy:     {final_val_acc:.4f} ({final_val_acc*100:.2f}%)              â•‘\nâ•‘  Validation Precision:    {final_val_precision:.4f} ({final_val_precision*100:.2f}%)              â•‘\nâ•‘  Validation Recall:       {final_val_recall:.4f} ({final_val_recall*100:.2f}%)              â•‘\nâ•‘  Validation F1-Score:     {f1_score:.4f} ({f1_score*100:.2f}%)              â•‘\nâ•‘  ROC AUC Score:           {roc_auc:.4f} ({roc_auc*100:.2f}%)              â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘  Sensitivity (Recall):    {sensitivity:.4f} ({sensitivity*100:.2f}%)              â•‘\nâ•‘  Specificity:             {specificity:.4f} ({specificity*100:.2f}%)              â•‘\nâ•‘  Positive Predictive Val: {ppv:.4f} ({ppv*100:.2f}%)              â•‘\nâ•‘  Negative Predictive Val: {npv:.4f} ({npv*100:.2f}%)              â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘  Total Training Epochs:   {total_epochs}                                  â•‘\nâ•‘  Best Epoch:              {best_epoch}                                  â•‘\nâ•‘  Training Samples:        {train_generator.samples}                               â•‘\nâ•‘  Validation Samples:      {validation_generator.samples}                               â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")\n\nprint(\"âœ… Evaluation completed!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T02:11:54.792820Z","iopub.execute_input":"2025-10-28T02:11:54.793090Z","iopub.status.idle":"2025-10-28T02:12:44.165497Z","shell.execute_reply.started":"2025-10-28T02:11:54.793071Z","shell.execute_reply":"2025-10-28T02:12:44.164555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================================================\n# ONNX CONVERSION - BREAST CANCER CLASSIFIER\n# ==================================================================================\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport tf2onnx\nimport onnx\nimport onnxruntime as ort\n\nprint(\"=\" * 80)\nprint(\"ONNX MODEL CONVERSION\")\nprint(\"=\" * 80)\n\n# ==================================================================================\n# STEP 1: SAVE KERAS MODEL\n# ==================================================================================\n\nprint(\"\\nğŸ“¦ Step 1: Saving Keras model...\")\n\n# Save the final trained model\nmodel.save('breast_cancer_classifier.h5')\nprint(\"âœ… Keras model saved as 'breast_cancer_classifier.h5'\")\n\n# Also save in SavedModel format\ntf.saved_model.save(model, 'breast_cancer_savedmodel')\nprint(\"âœ… SavedModel saved to 'breast_cancer_savedmodel/'\")\n\n# ==================================================================================\n# STEP 2: CONVERT TO ONNX\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONVERTING TO ONNX FORMAT\")\nprint(\"=\" * 80)\n\ntry:\n    print(\"\\nğŸ”„ Converting model to ONNX...\")\n    \n    # Define input specification\n    spec = tf.TensorSpec([None, 224, 224, 3], tf.float32, name='input')\n    \n    # Convert to ONNX\n    model_proto, _ = tf2onnx.convert.from_keras(\n        model,\n        input_signature=[spec],\n        opset=13,\n        output_path='breast_cancer_classifier.onnx'\n    )\n    \n    print(\"âœ… ONNX conversion successful!\")\n    \n    # Check file size\n    if os.path.exists('breast_cancer_classifier.onnx'):\n        file_size = os.path.getsize('breast_cancer_classifier.onnx') / (1024 * 1024)\n        print(f\"ğŸ“Š ONNX model size: {file_size:.2f} MB\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error during ONNX conversion: {str(e)}\")\n    print(\"\\nTrying alternative method...\")\n    \n    # Alternative method: Convert from SavedModel\n    try:\n        import subprocess\n        result = subprocess.run([\n            'python', '-m', 'tf2onnx.convert',\n            '--saved-model', 'breast_cancer_savedmodel',\n            '--output', 'breast_cancer_classifier.onnx',\n            '--opset', '13'\n        ], capture_output=True, text=True)\n        \n        print(result.stdout)\n        if result.returncode == 0:\n            print(\"âœ… Alternative method successful!\")\n        else:\n            print(result.stderr)\n    except Exception as e2:\n        print(f\"âŒ Alternative method failed: {str(e2)}\")\n\n# ==================================================================================\n# STEP 3: VALIDATE ONNX MODEL\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VALIDATING ONNX MODEL\")\nprint(\"=\" * 80)\n\nif os.path.exists('breast_cancer_classifier.onnx'):\n    try:\n        print(\"\\nğŸ” Loading ONNX model...\")\n        \n        # Load ONNX model\n        onnx_model = onnx.load('breast_cancer_classifier.onnx')\n        \n        # Check model\n        onnx.checker.check_model(onnx_model)\n        print(\"âœ… ONNX model is valid!\")\n        \n        # Print model info\n        print(f\"\\nğŸ“‹ Model Information:\")\n        print(f\"  - IR Version: {onnx_model.ir_version}\")\n        print(f\"  - Producer: {onnx_model.producer_name}\")\n        print(f\"  - Opset Version: {onnx_model.opset_import[0].version}\")\n        \n        # Get input/output info\n        print(f\"\\nğŸ“¥ Input Information:\")\n        for input_tensor in onnx_model.graph.input:\n            print(f\"  - Name: {input_tensor.name}\")\n            shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic' \n                    for dim in input_tensor.type.tensor_type.shape.dim]\n            print(f\"  - Shape: {shape}\")\n            print(f\"  - Type: {input_tensor.type.tensor_type.elem_type}\")\n        \n        print(f\"\\nğŸ“¤ Output Information:\")\n        for output_tensor in onnx_model.graph.output:\n            print(f\"  - Name: {output_tensor.name}\")\n            shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic' \n                    for dim in output_tensor.type.tensor_type.shape.dim]\n            print(f\"  - Shape: {shape}\")\n        \n    except Exception as e:\n        print(f\"âŒ Error validating ONNX model: {str(e)}\")\n\n# ==================================================================================\n# STEP 4: TEST ONNX INFERENCE\n# ==================================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING ONNX INFERENCE\")\nprint(\"=\" * 80)\n\nif os.path.exists('breast_cancer_classifier.onnx'):\n    try:\n        print(\"\\nğŸ§ª Running inference test...\")\n        \n        # Create ONNX Runtime session\n        ort_session = ort.InferenceSession('breast_cancer_classifier.onnx')\n        \n        # Get input name\n        input_name = ort_session.get_inputs()[0].name\n        output_name = ort_session.get_outputs()[0].name\n        \n        print(f\"  - Input name: {input_name}\")\n        print(f\"  - Output name: {output_name}\")\n        \n        # Create test input\n        test_input = np.random.random((1, 224, 224, 3)).astype(np.float32)\n        \n        # Run inference with ONNX\n        onnx_output = ort_session.run([output_name], {input_name: test_input})\n        \n        # Run inference with Keras\n        keras_output = model.predict(test_input, verbose=0)\n        \n        print(f\"\\nğŸ“Š Inference Comparison:\")\n        print(f\"  - Keras output: {keras_output[0][0]:.6f}\")\n        print(f\"  - ONNX output:  {onnx_output[0][0][0]:.6f}\")\n        print(f\"  - Difference:   {abs(keras_output[0][0] - onnx_output[0][0][0]):.8f}\")\n        \n        # Check if outputs match\n        if abs(keras_output[0][0] - onnx_output[0][0][0]) < 0.01:\n            print(\"\\nâœ… SUCCESS! ONNX model matches Keras model!\")\n        else:\n            print(\"\\nâš ï¸ Warning: Small difference detected between models\")\n        \n        # Test with real validation image\n        print(\"\\nğŸ–¼ï¸ Testing with real validation image...\")\n        validation_generator.reset()\n        real_image = next(validation_generator)[0][0:1]  # Get first image\n        \n        # Keras prediction\n        keras_pred = model.predict(real_image, verbose=0)\n        \n        # ONNX prediction\n        onnx_pred = ort_session.run([output_name], {input_name: real_image})\n        \n        print(f\"\\nğŸ“Š Real Image Test:\")\n        print(f\"  - Keras prediction: {keras_pred[0][0]:.6f} ({'Malignant' if keras_pred[0][0] > 0.5 else 'Benign'})\")\n        print(f\"  - ONNX prediction:  {onnx_pred[0][0][0]:.6f} ({'Malignant' if onnx_pred[0][0][0] > 0.5 else 'Benign'})\")\n        print(f\"  - Match: {'âœ… Yes' if abs(keras_pred[0][0] - onnx_pred[0][0][0]) < 0.01 else 'âŒ No'}\")\n        \n    except Exception as e:\n        print(f\"âŒ Error during inference test: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T02:15:30.962957Z","iopub.execute_input":"2025-10-28T02:15:30.963284Z","iopub.status.idle":"2025-10-28T02:16:52.298266Z","shell.execute_reply.started":"2025-10-28T02:15:30.963262Z","shell.execute_reply":"2025-10-28T02:16:52.297460Z"}},"outputs":[],"execution_count":null}]}